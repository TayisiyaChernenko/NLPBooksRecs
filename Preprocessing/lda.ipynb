{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "READ IN CLEANED FILE FROM S3\n",
    "'''\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define S3 bucket and file key\n",
    "bucket_name = 'abbynlpproject'\n",
    "file_key = 'cleaned_goodreads_reviews.csv'\n",
    "\n",
    "# Download the file\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "df = pd.read_csv(StringIO(obj['Body'].read().decode('utf-8')))\n",
    "\n",
    "# Check the first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in 'cleaned_review_text': 4894\n",
      "Proportion of NaN values in 'cleaned_review_text': 0.0014\n"
     ]
    }
   ],
   "source": [
    "# Count the number of NaN values in the 'cleaned_review_text' column\n",
    "nan_count = df['cleaned_review_text'].isna().sum()\n",
    "print(f\"Number of NaN values in 'cleaned_review_text': {nan_count}\")\n",
    "\n",
    "# Calculate the proportion with higher precision\n",
    "nan_proportion = df['cleaned_review_text'].isna().sum() / len(df)\n",
    "print(f\"Proportion of NaN values in 'cleaned_review_text': {nan_proportion:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(171232, 3)\n"
     ]
    }
   ],
   "source": [
    "# Sample a subset of the data (e.g., 10% of the data)\n",
    "sample_fraction = 0.05  # Adjust this based on your available resources\n",
    "df_sampled = df.sample(frac=sample_fraction, random_state=42)\n",
    "\n",
    "# Check the new size\n",
    "print(df_sampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining English reviews: 149169\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Function to detect if a review is in English\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'  # Return True if the text is in English\n",
    "    except:\n",
    "        return False  # Handle empty or error cases\n",
    "\n",
    "# Drop missing reviews and filter non-English reviews\n",
    "df_sampled = df_sampled.dropna(subset=['cleaned_review_text'])\n",
    "df_sampled = df_sampled[df_sampled['cleaned_review_text'].apply(is_english)]\n",
    "\n",
    "print(f\"Remaining English reviews: {len(df_sampled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity Score: 3372.0660480062384\n",
      "Top Words in Each Topic:\n",
      "Topic 1: life fantasy time novel like read world character story book\n",
      "Topic 2: character love good loved fun series great story read book\n",
      "Topic 3: im series didnt good story read character like really book\n",
      "Topic 4: amazing harry come love loved wait read series review book\n",
      "Topic 5: loved know character read really story like series love book\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "TOPICS for genre/dataset in GENERAL, not by book\n",
    "before removing common words\n",
    "'''\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Drop missing reviews\n",
    "df_sampled = df_sampled.dropna(subset=['cleaned_review_text'])\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X = vectorizer.fit_transform(df_sampled['cleaned_review_text'])\n",
    "\n",
    "# Function to compute coherence score (top words per topic)\n",
    "def compute_coherence_score(model, vectorizer):\n",
    "    topics = model.components_\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    coherence = []\n",
    "\n",
    "    for topic in topics:\n",
    "        topic_terms = [terms[i] for i in topic.argsort()[-10:]]  # Get top 10 words\n",
    "        coherence.append(' '.join(topic_terms))\n",
    "\n",
    "    return coherence\n",
    "\n",
    "# Train LDA with 5 topics\n",
    "lda_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda_model.fit(X)\n",
    "\n",
    "# Compute Perplexity\n",
    "perplexity_score = lda_model.perplexity(X)\n",
    "\n",
    "# Compute Coherence\n",
    "coherence_score = compute_coherence_score(lda_model, vectorizer)\n",
    "\n",
    "# Print results\n",
    "print(f\"Perplexity Score: {perplexity_score}\")\n",
    "print(\"Top Words in Each Topic:\")\n",
    "for i, topic in enumerate(coherence_score):\n",
    "    print(f\"Topic {i+1}: {topic}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Topics:\n",
      "Topic 1: felt liked\n",
      "Topic 2: dont\n",
      "Topic 3: favorite looking looking forward fantasy great forward\n",
      "Topic 4: way life world\n",
      "Topic 5: great come review come loved review\n",
      "Topic 6: great amazing loved\n",
      "Topic 7: fairy harry potter potter tale short fun harry\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "TOPICS for genre/dataset in GENERAL, not by book\n",
    "after removing common words\n",
    "'''\n",
    "\n",
    "# Define a custom list of words to remove\n",
    "custom_stop_words = {'book', 'story', 'series', 'read', 'novel', 'character', 'love', 'really', 'like', 'time', 'good', 'know', 'im', 'didnt', 'wait'}\n",
    "\n",
    "# Initialize TF-IDF Vectorizer with improved parameters\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             max_df=0.8, \n",
    "                             min_df=5,  # Ignore words that appear in fewer than 5 reviews\n",
    "                             max_features=10000,  # Increase vocab size for richer topics\n",
    "                             ngram_range=(1,2))  # Capture word pairs\n",
    "\n",
    "X = vectorizer.fit_transform(df_sampled['cleaned_review_text'])\n",
    "\n",
    "# Function to filter out generic words from topics\n",
    "def clean_topics(topics, stop_words):\n",
    "    cleaned_topics = []\n",
    "    for topic in topics:\n",
    "        topic_terms = topic.split()\n",
    "        filtered_terms = [word for word in topic_terms if word not in stop_words]\n",
    "        cleaned_topics.append(' '.join(filtered_terms))\n",
    "    return cleaned_topics\n",
    "\n",
    "# Train LDA with more topics\n",
    "lda_model = LatentDirichletAllocation(n_components=7, random_state=42)  # More topics for diversity\n",
    "lda_model.fit(X)\n",
    "\n",
    "# Compute Coherence\n",
    "coherence_score = compute_coherence_score(lda_model, vectorizer)\n",
    "\n",
    "# Remove generic words from topics\n",
    "filtered_topics = clean_topics(coherence_score, custom_stop_words)\n",
    "\n",
    "# Print results\n",
    "print(\"Filtered Topics:\")\n",
    "for i, topic in enumerate(filtered_topics):\n",
    "    print(f\"Topic {i+1}: {topic}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book ID: 1\n",
      "  Topic 1: forever need dumbledore 2013 15th ring professor excited sir slow\n",
      "  Topic 2: really review great read movie potter dumbledore reading harry book\n",
      "  Topic 3: really read love best time favorite potter series harry book\n",
      "\n",
      "\n",
      "Book ID: 2\n",
      "  Topic 1: good character umbridge make love series story harry like book\n",
      "  Topic 2: stupid ride angst like baby teen came didnt review httpswwwyoutubecomwatchv68ne\n",
      "  Topic 3: sirius really loved series time favorite potter read harry book\n",
      "\n",
      "\n",
      "Book ID: 3\n",
      "  Topic 1: 2nd coming enjoying reathon beautiful writer thsi special shes chapter\n",
      "  Topic 2: reread good great time potter series read harry love book\n",
      "  Topic 3: loved series like potter time im harry reading read book\n",
      "\n",
      "\n",
      "Book ID: 6\n",
      "  Topic 1: love year second harry series loved time read favorite book\n",
      "  Topic 2: love know page far series like favourite potter harry book\n",
      "  Topic 3: story exciting time best love potter read series harry book\n",
      "\n",
      "\n",
      "Book ID: 11\n",
      "  Topic 1: clever time funny reread got ago year story book read\n",
      "  Topic 2: mean im rereading funny human life like read really book\n",
      "  Topic 3: loved time im dont enjoyable read novel classic really book\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Set minimum number of reviews per book\n",
    "min_reviews = 10  \n",
    "\n",
    "# Process only the first 500 books for proof of concept\n",
    "subset_books = df_sampled['book_id'].value_counts().index[:500]  \n",
    "df_filtered = df_sampled[df_sampled['book_id'].isin(subset_books)]\n",
    "\n",
    "# Function to process a single book\n",
    "def process_book(book_id, group):\n",
    "    if len(group) < min_reviews:\n",
    "        return book_id, []  # Skip books with too few reviews\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=3000)\n",
    "    X = vectorizer.fit_transform(group['cleaned_review_text'])\n",
    "\n",
    "    lda_model = LatentDirichletAllocation(n_components=3, random_state=42)  # Only 3 topics per book for speed\n",
    "    lda_model.fit(X)\n",
    "\n",
    "    # Extract topics\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        top_words = [terms[i] for i in topic.argsort()[-10:]]  # Top 10 words per topic\n",
    "        topics.append(\" \".join(top_words))\n",
    "\n",
    "    return book_id, topics\n",
    "\n",
    "# Run topic modeling in parallel\n",
    "book_groups = df_filtered.groupby('book_id')\n",
    "results = Parallel(n_jobs=-1)(delayed(process_book)(book_id, group) for book_id, group in book_groups)\n",
    "\n",
    "# Convert results to dictionary\n",
    "book_topics = {book_id: topics for book_id, topics in results if topics}\n",
    "\n",
    "# Print results for the first 5 books\n",
    "for book, topics in list(book_topics.items())[:5]:\n",
    "    print(f\"Book ID: {book}\")\n",
    "    for i, topic in enumerate(topics):\n",
    "        print(f\"  Topic {i+1}: {topic}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book Title: Harry Potter and the Half-Blood Prince (Harry Potter, #6) (Book ID: 1)\n",
      "  Topic 1: 1st spoiler snape better voldemort started tyrant dumbledore excited movie\n",
      "  Topic 2: setup prince know im rowling againgeez potter definitely harry dumbledore\n",
      "  Topic 3: end im magic think eternity believe reread harry\n",
      "  Topic 4: right enjoyed think rereading second dumbledore feel potter harry\n",
      "  Topic 5: obviously reading favourite absolute far potter harry\n",
      "\n",
      "\n",
      "Book Title: Harry Potter and the Order of the Phoenix (Harry Potter, #5) (Book ID: 2)\n",
      "  Topic 1: long make old end year character harry reading\n",
      "  Topic 2: reread actually 5th fail spoiler attention came sad capture believe\n",
      "  Topic 3: ive character im sirius loved novel potter harry\n",
      "  Topic 4: finished darker excellent thing definitely reread potter harry\n",
      "  Topic 5: awesome make potter umbridge teenage rowling new angst movie harry\n",
      "\n",
      "\n",
      "Book Title: Harry Potter and the Sorcerer's Stone (Harry Potter, #1) (Book ID: 3)\n",
      "  Topic 1: come classroomlibrary special didnt forever amazing potter reading harry\n",
      "  Topic 2: movie kid know im loved potter harry\n",
      "  Topic 3: come better say magical rowling reread potter reading loved harry\n",
      "  Topic 4: ive think character ago start year rereading reread amazing\n",
      "  Topic 5: year finished movie think ive im potter harry reading\n",
      "\n",
      "\n",
      "Book Title: Harry Potter and the Goblet of Fire (Harry Potter, #4) (Book ID: 6)\n",
      "  Topic 1: voldemort wasnt cup wonderful world loved year got harry\n",
      "  Topic 2: longer awesome set thing course absolutely doubt better potter harry\n",
      "  Topic 3: end cried movie excellent potter harry point\n",
      "  Topic 4: enjoyed didnt came invokes shouldve gose reading magic harry potter\n",
      "  Topic 5: movie reread potter far loved favourite harry\n",
      "\n",
      "\n",
      "Book Title: The Hitchhiker's Guide to the Galaxy (Hitchhiker's Guide to the Galaxy, #1) (Book ID: 11)\n",
      "  Topic 1: intelligent finally sure classic far hilarious lot fish funny 45\n",
      "  Topic 2: eternal insightful favourite im existentialism know race funny life human\n",
      "  Topic 3: humorous dont hitchhiker loved adam galaxy novel humor classic\n",
      "  Topic 4: listening ago funny enjoyable lost liked written audio fun\n",
      "  Topic 5: rereading felt ive im easy laugh currently club enjoyable loved\n",
      "\n",
      "\n",
      "Book Title: The Ultimate Hitchhiker's Guide to the Galaxy (Book ID: 13)\n",
      "  Topic 1: cosmic mangled mutated covered messy explosion leaf charm second funny\n",
      "  Topic 2: coming thing instance big mind quote delightful make laugh hard\n",
      "  Topic 3: odd greatest loud joke comedy science stop fiction probably humour\n",
      "  Topic 4: fantastical quirky entertaining fun bring wish encore trilogy adam author\n",
      "  Topic 5: thats hilarious humor describing ground style awesome year way adam\n",
      "\n",
      "\n",
      "Book Title: The Lord of the Rings (The Lord of the Rings, #1-3) (Book ID: 33)\n",
      "  Topic 1: said writing tolkien ive trilogy movie reading epic lotr say\n",
      "  Topic 2: movie landscape epic earth middle hobbit world reading im mind\n",
      "  Topic 3: omnibus extra ed 3in1 took half nearly loved kid use\n",
      "  Topic 4: movie ring going better ive film hobbit adventure way trilogy\n",
      "  Topic 5: hard follow multitude far plot iti piece work greatest awesome\n",
      "\n",
      "\n",
      "Book Title: The Fellowship of the Ring (The Lord of the Rings, #1) (Book ID: 34)\n",
      "  Topic 1: felt recommend quite novel character ring fully fantasy reading\n",
      "  Topic 2: bored world star tolkien reading way includes year liked movie\n",
      "  Topic 3: favoriteand world ive tolkien complicated lord understand fivestar long know\n",
      "  Topic 4: life fantastic incredibly world writing language audiobook doubt listened year\n",
      "  Topic 5: hobbit based teaching fair think style finish stand listened started\n",
      "\n",
      "\n",
      "Book Title: The Phantom Tollbooth (Book ID: 378)\n",
      "  Topic 1: tollbooth older enjoy younger 36 100 nea educator im childrens\n",
      "  Topic 2: boy reading enjoyed delightful age amazing wish wordplay little\n",
      "  Topic 3: setting 5th educational imagery kidsschool loved think grade image yeah\n",
      "  Topic 4: purge lucky mama survived mother hilarious recently childrens clever alltime\n",
      "  Topic 5: written killer cool day people child funny imaginative classic reread\n",
      "\n",
      "\n",
      "Book Title: A Midsummer Night's Dream (Book ID: 1622)\n",
      "  Topic 1: word grade clearly 7th magical fairy mystical favourite work shakespeare\n",
      "  Topic 2: bye dream enjoyed impression hardback switching person confused thoroughly shakespeare\n",
      "  Topic 3: worth popular consider known lesser bitmore expecting vaguely hype shakespeare\n",
      "  Topic 4: enjoyed different nearly think delight gold flurry fun reading play\n",
      "  Topic 5: compared isnt dont young forest course havent day sooooooooooooo finished\n",
      "\n",
      "\n",
      "Book Title: Anansi Boys (Book ID: 2744)\n",
      "  Topic 1: lenny unique narrator think listened short car enjoyed gaiman way\n",
      "  Topic 2: modern gaiman interesting prize look african gaimans version anansi god\n",
      "  Topic 3: told holt tom style brother neil gaiman american spider fat\n",
      "  Topic 4: inside hilarious little storytelling loved fucken actually american better intriguing\n",
      "  Topic 5: theme dont neil didnt character quite american think im gaiman\n",
      "\n",
      "\n",
      "Book Title: A Great and Terrible Beauty (Gemma Doyle, #1) (Book ID: 3682)\n",
      "  Topic 1: school girl mother dont victorian think fantasy gemma\n",
      "  Topic 2: think enjoy quite doyle wait tried store star teen hmm\n",
      "  Topic 3: took reason toe bit character continue dont dragged liked historical\n",
      "  Topic 4: liked say world teenage commend coming hard dimensional girl soon\n",
      "  Topic 5: plot thing period quite page liked school girl confusing character\n",
      "\n",
      "\n",
      "Book Title: Life of Pi (Book ID: 4214)\n",
      "  Topic 1: reading say beautifully amazing chapter life written adventure pi\n",
      "  Topic 2: getting fantastic enjoyed thought interesting finish stand sea little amazing\n",
      "  Topic 3: quite better concept bit remember character movie different pi want\n",
      "  Topic 4: loved fantastic film felt enjoyed worth start life im novel\n",
      "  Topic 5: worth dont sure wonderful end ending lifeboat tiger pi\n",
      "\n",
      "\n",
      "Book Title: American Gods (American Gods, #1) (Book ID: 4407)\n",
      "  Topic 1: halfway neil httpmizparkerwordpresscom20110 character new american gaiman reading god\n",
      "  Topic 2: enjoyable fucking gaiman phew hard felt 35 long im god\n",
      "  Topic 3: character shadow thought im little concept interesting gaiman definitely god\n",
      "  Topic 4: coming liked novel think gaiman enjoyed character god\n",
      "  Topic 5: character ending sure interesting weird neil american god thought finish\n",
      "\n",
      "\n",
      "Book Title: The Dark Tower (Book ID: 5091)\n",
      "  Topic 1: ending im wonderful happens pretty writing king end dark tower\n",
      "  Topic 2: loved dark people hated know previous piss way reader okay\n",
      "  Topic 3: idea want actually amazing king thing forever hate saga sense\n",
      "  Topic 4: actually ended rest hope opus digest ending magnus excellent bit\n",
      "  Topic 5: wont absolutely id enthusiastically promulgate unfulfilling asked dark tower ending\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Set minimum number of reviews per book\n",
    "min_reviews = 10  \n",
    "\n",
    "# Process only the first 500 books for proof of concept\n",
    "subset_books = df_sampled['book_id'].value_counts().index[:1000]  \n",
    "df_filtered = df_sampled[df_sampled['book_id'].isin(subset_books)]\n",
    "\n",
    "# Define a custom list of generic words to remove\n",
    "custom_stopwords = set([\n",
    "    \"book\", \"read\", \"series\", \"time\", \"love\", \"story\", \"like\", \n",
    "    \"really\", \"good\", \"great\", \"best\", \"favorite\", \"review\"\n",
    "])\n",
    "\n",
    "# Function to process a single book\n",
    "def process_book(book_id, group):\n",
    "    if len(group) < min_reviews:\n",
    "        return book_id, None  # Skip books with too few reviews\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=3000)\n",
    "    X = vectorizer.fit_transform(group['cleaned_review_text'])\n",
    "\n",
    "    lda_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "    lda_model.fit(X)\n",
    "\n",
    "    # Extract topics\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        top_words = [terms[i] for i in topic.argsort()[-15:]]  # Get more words per topic\n",
    "        filtered_words = [word for word in top_words if word not in custom_stopwords]\n",
    "        topics.append(\" \".join(filtered_words[:10]))  # Keep only top 10 after filtering\n",
    "\n",
    "    # Get book title\n",
    "    title = group['title'].iloc[0]  # Assuming all reviews for a book have the same title\n",
    "\n",
    "    return book_id, title, topics\n",
    "\n",
    "# Run topic modeling in parallel\n",
    "book_groups = df_filtered.groupby('book_id')\n",
    "results = Parallel(n_jobs=-1)(delayed(process_book)(book_id, group) for book_id, group in book_groups)\n",
    "\n",
    "# Convert results to dictionary\n",
    "book_topics = {book_id: (title, topics) for book_id, title, topics in results if topics}\n",
    "\n",
    "# Print results for the first 5 books\n",
    "for book_id, (title, topics) in list(book_topics.items())[:15]:\n",
    "    print(f\"Book Title: {title} (Book ID: {book_id})\")\n",
    "    for i, topic in enumerate(topics):\n",
    "        print(f\"  Topic {i+1}: {topic}\")\n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
