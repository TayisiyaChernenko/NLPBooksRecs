{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['user_id', 'book_id', 'review_id', 'rating', 'review_text',\n",
      "       'date_added', 'date_updated', 'read_at', 'started_at', 'n_votes',\n",
      "       'n_comments'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import gzip\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define your bucket name and file path\n",
    "bucket_name = 'abbynlpproject'\n",
    "file_key = 'goodreads_reviews_fantasy_paranormal.json.gz'\n",
    "\n",
    "# Download the file from S3 to local memory\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "\n",
    "# Decompress the .gz file\n",
    "with gzip.GzipFile(fileobj=BytesIO(obj['Body'].read()), mode='rb') as f:\n",
    "    # Read the file line by line (assuming each line is a separate JSON object)\n",
    "    data = []\n",
    "    for line in f:\n",
    "        try:\n",
    "            # Decode each line, parse it as JSON and append to the data list\n",
    "            data.append(json.loads(line.decode('utf-8')))\n",
    "        except json.JSONDecodeError:\n",
    "            continue  # Skip any malformed lines or invalid JSON objects\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Check the columns of the book metadata dataframe\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    book_id                                        review_text  rating\n",
      "0  18245960  This is a special book. It started slow for ab...       5\n",
      "1   5577844  A beautiful story. Neil Gaiman is truly a uniq...       5\n",
      "2  17315048  Mark Watney is a steely-eyed missile man. A ma...       5\n",
      "3  13453029  A fun fast paced book that sucks you in right ...       4\n",
      "4  13239822  This book has a great premise, and is full of ...       3\n"
     ]
    }
   ],
   "source": [
    "# Keep only the 'book_id' and 'review_text' columns\n",
    "df_cleaned = df[['book_id', 'review_text', 'rating']]\n",
    "\n",
    "# Display the cleaned dataframe\n",
    "print(df_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['isbn', 'text_reviews_count', 'series', 'country_code', 'language_code',\n",
      "       'popular_shelves', 'asin', 'is_ebook', 'average_rating', 'kindle_asin',\n",
      "       'similar_books', 'description', 'format', 'link', 'authors',\n",
      "       'publisher', 'num_pages', 'publication_day', 'isbn13',\n",
      "       'publication_month', 'edition_information', 'publication_year', 'url',\n",
      "       'image_url', 'book_id', 'ratings_count', 'work_id', 'title',\n",
      "       'title_without_series'],\n",
      "      dtype='object')\n",
      "                            user_id   book_id  \\\n",
      "0  8842281e1d1347389f2ab93d60773d4d  18245960   \n",
      "1  8842281e1d1347389f2ab93d60773d4d   5577844   \n",
      "2  8842281e1d1347389f2ab93d60773d4d  17315048   \n",
      "3  8842281e1d1347389f2ab93d60773d4d  13453029   \n",
      "4  8842281e1d1347389f2ab93d60773d4d  13239822   \n",
      "\n",
      "                          review_id  rating  \\\n",
      "0  dfdbb7b0eb5a7e4c26d59a937e2e5feb       5   \n",
      "1  52c8ac49496c153e4a97161e36b2db55       5   \n",
      "2  885c772fb033b041f42d57cef5be0a43       5   \n",
      "3  46a6e1a14e8afc82d221fec0a2bd3dd0       4   \n",
      "4  a582bfa8efd69d453a5a21a678046b36       3   \n",
      "\n",
      "                                         review_text  \\\n",
      "0  This is a special book. It started slow for ab...   \n",
      "1  A beautiful story. Neil Gaiman is truly a uniq...   \n",
      "2  Mark Watney is a steely-eyed missile man. A ma...   \n",
      "3  A fun fast paced book that sucks you in right ...   \n",
      "4  This book has a great premise, and is full of ...   \n",
      "\n",
      "                       date_added                    date_updated  \\\n",
      "0  Sun Jul 30 07:44:10 -0700 2017  Wed Aug 30 00:00:26 -0700 2017   \n",
      "1  Wed Sep 24 09:29:29 -0700 2014  Wed Oct 01 00:31:56 -0700 2014   \n",
      "2  Sat Apr 05 09:30:53 -0700 2014  Wed Mar 22 11:33:10 -0700 2017   \n",
      "3  Tue Dec 04 11:12:22 -0800 2012  Sat Jul 26 11:43:28 -0700 2014   \n",
      "4  Mon Jul 02 16:04:16 -0700 2012  Wed Mar 22 11:32:20 -0700 2017   \n",
      "\n",
      "                          read_at                      started_at  n_votes  \\\n",
      "0  Sat Aug 26 12:05:52 -0700 2017  Tue Aug 15 13:23:18 -0700 2017       28   \n",
      "1  Tue Sep 30 00:00:00 -0700 2014  Sun Sep 21 00:00:00 -0700 2014        5   \n",
      "2  Mon Aug 25 00:00:00 -0700 2014  Sat Aug 16 00:00:00 -0700 2014       25   \n",
      "3  Tue Jul 08 00:00:00 -0700 2014  Wed Jul 02 00:00:00 -0700 2014        5   \n",
      "4  Wed Aug 15 00:00:00 -0700 2012  Sun Aug 12 00:00:00 -0700 2012        7   \n",
      "\n",
      "   n_comments                                              title  \n",
      "0           1  The Three-Body Problem (Remembrance of Earth’s...  \n",
      "1           1                                           Stardust  \n",
      "2           5                                        The Martian  \n",
      "3           1                            Wool Omnibus (Silo, #1)  \n",
      "4           0                                    Alif the Unseen  \n"
     ]
    }
   ],
   "source": [
    "# Define your file key for the book metadata\n",
    "book_metadata_key = 'goodreads_books_fantasy_paranormal.json.gz'\n",
    "\n",
    "# Download the book metadata file from S3\n",
    "book_metadata_obj = s3.get_object(Bucket=bucket_name, Key=book_metadata_key)\n",
    "\n",
    "# Decompress and load the book metadata (similar to the reviews data)\n",
    "with gzip.GzipFile(fileobj=BytesIO(book_metadata_obj['Body'].read()), mode='rb') as f:\n",
    "    book_metadata = []\n",
    "    for line in f:\n",
    "        try:\n",
    "            # Decode and parse the JSON data\n",
    "            book_metadata.append(json.loads(line.decode('utf-8')))\n",
    "        except json.JSONDecodeError:\n",
    "            continue  # Skip any malformed lines\n",
    "\n",
    "# Convert the book metadata list to a DataFrame\n",
    "df_books = pd.DataFrame(book_metadata)\n",
    "\n",
    "# Check the columns of the book metadata dataframe\n",
    "print(df_books.columns)\n",
    "\n",
    "# Assuming the column name is 'title' (or whatever is appropriate), use that to merge\n",
    "df_with_book_names = pd.merge(df, df_books[['book_id', 'title']], on='book_id', how='left')\n",
    "\n",
    "# Display the first few rows of the merged DataFrame\n",
    "print(df_with_book_names.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/abbyeast/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/abbyeast/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         review_text  \\\n",
      "0  This is a special book. It started slow for ab...   \n",
      "1  A beautiful story. Neil Gaiman is truly a uniq...   \n",
      "2  Mark Watney is a steely-eyed missile man. A ma...   \n",
      "3  A fun fast paced book that sucks you in right ...   \n",
      "4  This book has a great premise, and is full of ...   \n",
      "\n",
      "                                 cleaned_review_text  \n",
      "0  special book started slow first third middle t...  \n",
      "1  beautiful story neil gaiman truly unique story...  \n",
      "2  mark watney steelyeyed missile man man man bad...  \n",
      "3  fun fast paced book suck right away doesnt let...  \n",
      "4  book great premise full beautifully written pr...  \n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download the stopwords and lemmatizer if you haven't already\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenize the text (split it into words)\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords and lemmatize the remaining words\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    # Join the words back into a single string\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply the preprocessing function to the review_text column\n",
    "df_with_book_names['cleaned_review_text'] = df_with_book_names['review_text'].apply(preprocess_text)\n",
    "\n",
    "# Check the first few rows after preprocessing\n",
    "print(df_with_book_names[['review_text', 'cleaned_review_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>book_id</th>\n",
       "      <th>cleaned_review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Three-Body Problem (Remembrance of Earth’s...</td>\n",
       "      <td>18245960</td>\n",
       "      <td>special book started slow first third middle t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stardust</td>\n",
       "      <td>5577844</td>\n",
       "      <td>beautiful story neil gaiman truly unique story...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Martian</td>\n",
       "      <td>17315048</td>\n",
       "      <td>mark watney steelyeyed missile man man man bad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wool Omnibus (Silo, #1)</td>\n",
       "      <td>13453029</td>\n",
       "      <td>fun fast paced book suck right away doesnt let...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alif the Unseen</td>\n",
       "      <td>13239822</td>\n",
       "      <td>book great premise full beautifully written pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title   book_id  \\\n",
       "0  The Three-Body Problem (Remembrance of Earth’s...  18245960   \n",
       "1                                           Stardust   5577844   \n",
       "2                                        The Martian  17315048   \n",
       "3                            Wool Omnibus (Silo, #1)  13453029   \n",
       "4                                    Alif the Unseen  13239822   \n",
       "\n",
       "                                 cleaned_review_text  \n",
       "0  special book started slow first third middle t...  \n",
       "1  beautiful story neil gaiman truly unique story...  \n",
       "2  mark watney steelyeyed missile man man man bad...  \n",
       "3  fun fast paced book suck right away doesnt let...  \n",
       "4  book great premise full beautifully written pr...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df = df_with_book_names[['title', 'book_id', 'cleaned_review_text']]\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to S3: s3://abbynlpproject/cleaned_goodreads_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define S3 bucket and filename\n",
    "bucket_name = 'abbynlpproject'\n",
    "file_key = 'cleaned_goodreads_reviews.csv'\n",
    "\n",
    "# Convert DataFrame to CSV in memory\n",
    "csv_buffer = StringIO()\n",
    "cleaned_df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "# Upload to S3\n",
    "s3.put_object(Bucket=bucket_name, Key=file_key, Body=csv_buffer.getvalue())\n",
    "\n",
    "print(f\"File saved to S3: s3://{bucket_name}/{file_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title   book_id  \\\n",
      "0  The Three-Body Problem (Remembrance of Earth’s...  18245960   \n",
      "1                                           Stardust   5577844   \n",
      "2                                        The Martian  17315048   \n",
      "3                            Wool Omnibus (Silo, #1)  13453029   \n",
      "4                                    Alif the Unseen  13239822   \n",
      "\n",
      "                                 cleaned_review_text  \n",
      "0  special book started slow first third middle t...  \n",
      "1  beautiful story neil gaiman truly unique story...  \n",
      "2  mark watney steelyeyed missile man man man bad...  \n",
      "3  fun fast paced book suck right away doesnt let...  \n",
      "4  book great premise full beautifully written pr...  \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "READ IN CLEANED FILE FROM S3\n",
    "'''\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define S3 bucket and file key\n",
    "bucket_name = 'abbynlpproject'\n",
    "file_key = 'cleaned_goodreads_reviews.csv'\n",
    "\n",
    "# Download the file\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "df = pd.read_csv(StringIO(obj['Body'].read().decode('utf-8')))\n",
    "\n",
    "# Check the first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in 'cleaned_review_text': 4894\n",
      "Proportion of NaN values in 'cleaned_review_text': 0.0014\n"
     ]
    }
   ],
   "source": [
    "# Count the number of NaN values in the 'cleaned_review_text' column\n",
    "nan_count = df['cleaned_review_text'].isna().sum()\n",
    "print(f\"Number of NaN values in 'cleaned_review_text': {nan_count}\")\n",
    "\n",
    "# Calculate the proportion with higher precision\n",
    "nan_proportion = df['cleaned_review_text'].isna().sum() / len(df)\n",
    "print(f\"Proportion of NaN values in 'cleaned_review_text': {nan_proportion:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(171232, 3)\n"
     ]
    }
   ],
   "source": [
    "# Sample a subset of the data (e.g., 10% of the data)\n",
    "sample_fraction = 0.05  # Adjust this based on your available resources\n",
    "df_sampled = df.sample(frac=sample_fraction, random_state=42)\n",
    "\n",
    "# Check the new size\n",
    "print(df_sampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining English reviews: 149169\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Function to detect if a review is in English\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'  # Return True if the text is in English\n",
    "    except:\n",
    "        return False  # Handle empty or error cases\n",
    "\n",
    "# Drop missing reviews and filter non-English reviews\n",
    "df_sampled = df_sampled.dropna(subset=['cleaned_review_text'])\n",
    "df_sampled = df_sampled[df_sampled['cleaned_review_text'].apply(is_english)]\n",
    "\n",
    "print(f\"Remaining English reviews: {len(df_sampled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity Score: 3372.0660480062384\n",
      "Top Words in Each Topic:\n",
      "Topic 1: life fantasy time novel like read world character story book\n",
      "Topic 2: character love good loved fun series great story read book\n",
      "Topic 3: im series didnt good story read character like really book\n",
      "Topic 4: amazing harry come love loved wait read series review book\n",
      "Topic 5: loved know character read really story like series love book\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "TOPICS for genre/dataset in GENERAL, not by book\n",
    "before removing common words\n",
    "'''\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Drop missing reviews\n",
    "df_sampled = df_sampled.dropna(subset=['cleaned_review_text'])\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X = vectorizer.fit_transform(df_sampled['cleaned_review_text'])\n",
    "\n",
    "# Function to compute coherence score (top words per topic)\n",
    "def compute_coherence_score(model, vectorizer):\n",
    "    topics = model.components_\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    coherence = []\n",
    "\n",
    "    for topic in topics:\n",
    "        topic_terms = [terms[i] for i in topic.argsort()[-10:]]  # Get top 10 words\n",
    "        coherence.append(' '.join(topic_terms))\n",
    "\n",
    "    return coherence\n",
    "\n",
    "# Train LDA with 5 topics\n",
    "lda_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda_model.fit(X)\n",
    "\n",
    "# Compute Perplexity\n",
    "perplexity_score = lda_model.perplexity(X)\n",
    "\n",
    "# Compute Coherence\n",
    "coherence_score = compute_coherence_score(lda_model, vectorizer)\n",
    "\n",
    "# Print results\n",
    "print(f\"Perplexity Score: {perplexity_score}\")\n",
    "print(\"Top Words in Each Topic:\")\n",
    "for i, topic in enumerate(coherence_score):\n",
    "    print(f\"Topic {i+1}: {topic}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Topics:\n",
      "Topic 1: felt liked\n",
      "Topic 2: dont\n",
      "Topic 3: favorite looking looking forward fantasy great forward\n",
      "Topic 4: way life world\n",
      "Topic 5: great come review come loved review\n",
      "Topic 6: great amazing loved\n",
      "Topic 7: fairy harry potter potter tale short fun harry\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "TOPICS for genre/dataset in GENERAL, not by book\n",
    "after removing common words\n",
    "'''\n",
    "\n",
    "# Define a custom list of words to remove\n",
    "custom_stop_words = {'book', 'story', 'series', 'read', 'novel', 'character', 'love', 'really', 'like', 'time', 'good', 'know', 'im', 'didnt', 'wait'}\n",
    "\n",
    "# Initialize TF-IDF Vectorizer with improved parameters\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             max_df=0.8, \n",
    "                             min_df=5,  # Ignore words that appear in fewer than 5 reviews\n",
    "                             max_features=10000,  # Increase vocab size for richer topics\n",
    "                             ngram_range=(1,2))  # Capture word pairs\n",
    "\n",
    "X = vectorizer.fit_transform(df_sampled['cleaned_review_text'])\n",
    "\n",
    "# Function to filter out generic words from topics\n",
    "def clean_topics(topics, stop_words):\n",
    "    cleaned_topics = []\n",
    "    for topic in topics:\n",
    "        topic_terms = topic.split()\n",
    "        filtered_terms = [word for word in topic_terms if word not in stop_words]\n",
    "        cleaned_topics.append(' '.join(filtered_terms))\n",
    "    return cleaned_topics\n",
    "\n",
    "# Train LDA with more topics\n",
    "lda_model = LatentDirichletAllocation(n_components=7, random_state=42)  # More topics for diversity\n",
    "lda_model.fit(X)\n",
    "\n",
    "# Compute Coherence\n",
    "coherence_score = compute_coherence_score(lda_model, vectorizer)\n",
    "\n",
    "# Remove generic words from topics\n",
    "filtered_topics = clean_topics(coherence_score, custom_stop_words)\n",
    "\n",
    "# Print results\n",
    "print(\"Filtered Topics:\")\n",
    "for i, topic in enumerate(filtered_topics):\n",
    "    print(f\"Topic {i+1}: {topic}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book ID: 1\n",
      "  Topic 1: forever need dumbledore 2013 15th ring professor excited sir slow\n",
      "  Topic 2: really review great read movie potter dumbledore reading harry book\n",
      "  Topic 3: really read love best time favorite potter series harry book\n",
      "\n",
      "\n",
      "Book ID: 2\n",
      "  Topic 1: good character umbridge make love series story harry like book\n",
      "  Topic 2: stupid ride angst like baby teen came didnt review httpswwwyoutubecomwatchv68ne\n",
      "  Topic 3: sirius really loved series time favorite potter read harry book\n",
      "\n",
      "\n",
      "Book ID: 3\n",
      "  Topic 1: 2nd coming enjoying reathon beautiful writer thsi special shes chapter\n",
      "  Topic 2: reread good great time potter series read harry love book\n",
      "  Topic 3: loved series like potter time im harry reading read book\n",
      "\n",
      "\n",
      "Book ID: 6\n",
      "  Topic 1: love year second harry series loved time read favorite book\n",
      "  Topic 2: love know page far series like favourite potter harry book\n",
      "  Topic 3: story exciting time best love potter read series harry book\n",
      "\n",
      "\n",
      "Book ID: 11\n",
      "  Topic 1: clever time funny reread got ago year story book read\n",
      "  Topic 2: mean im rereading funny human life like read really book\n",
      "  Topic 3: loved time im dont enjoyable read novel classic really book\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Set minimum number of reviews per book\n",
    "min_reviews = 10  \n",
    "\n",
    "# Process only the first 500 books for proof of concept\n",
    "subset_books = df_sampled['book_id'].value_counts().index[:500]  \n",
    "df_filtered = df_sampled[df_sampled['book_id'].isin(subset_books)]\n",
    "\n",
    "# Function to process a single book\n",
    "def process_book(book_id, group):\n",
    "    if len(group) < min_reviews:\n",
    "        return book_id, []  # Skip books with too few reviews\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=3000)\n",
    "    X = vectorizer.fit_transform(group['cleaned_review_text'])\n",
    "\n",
    "    lda_model = LatentDirichletAllocation(n_components=3, random_state=42)  # Only 3 topics per book for speed\n",
    "    lda_model.fit(X)\n",
    "\n",
    "    # Extract topics\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        top_words = [terms[i] for i in topic.argsort()[-10:]]  # Top 10 words per topic\n",
    "        topics.append(\" \".join(top_words))\n",
    "\n",
    "    return book_id, topics\n",
    "\n",
    "# Run topic modeling in parallel\n",
    "book_groups = df_filtered.groupby('book_id')\n",
    "results = Parallel(n_jobs=-1)(delayed(process_book)(book_id, group) for book_id, group in book_groups)\n",
    "\n",
    "# Convert results to dictionary\n",
    "book_topics = {book_id: topics for book_id, topics in results if topics}\n",
    "\n",
    "# Print results for the first 5 books\n",
    "for book, topics in list(book_topics.items())[:5]:\n",
    "    print(f\"Book ID: {book}\")\n",
    "    for i, topic in enumerate(topics):\n",
    "        print(f\"  Topic {i+1}: {topic}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book Title: Harry Potter and the Half-Blood Prince (Harry Potter, #6) (Book ID: 1)\n",
      "  Topic 1: 1st spoiler snape better voldemort started tyrant dumbledore excited movie\n",
      "  Topic 2: setup prince know im rowling againgeez potter definitely harry dumbledore\n",
      "  Topic 3: end im magic think eternity believe reread harry\n",
      "  Topic 4: right enjoyed think rereading second dumbledore feel potter harry\n",
      "  Topic 5: obviously reading favourite absolute far potter harry\n",
      "\n",
      "\n",
      "Book Title: Harry Potter and the Order of the Phoenix (Harry Potter, #5) (Book ID: 2)\n",
      "  Topic 1: long make old end year character harry reading\n",
      "  Topic 2: reread actually 5th fail spoiler attention came sad capture believe\n",
      "  Topic 3: ive character im sirius loved novel potter harry\n",
      "  Topic 4: finished darker excellent thing definitely reread potter harry\n",
      "  Topic 5: awesome make potter umbridge teenage rowling new angst movie harry\n",
      "\n",
      "\n",
      "Book Title: Harry Potter and the Sorcerer's Stone (Harry Potter, #1) (Book ID: 3)\n",
      "  Topic 1: come classroomlibrary special didnt forever amazing potter reading harry\n",
      "  Topic 2: movie kid know im loved potter harry\n",
      "  Topic 3: come better say magical rowling reread potter reading loved harry\n",
      "  Topic 4: ive think character ago start year rereading reread amazing\n",
      "  Topic 5: year finished movie think ive im potter harry reading\n",
      "\n",
      "\n",
      "Book Title: Harry Potter and the Goblet of Fire (Harry Potter, #4) (Book ID: 6)\n",
      "  Topic 1: voldemort wasnt cup wonderful world loved year got harry\n",
      "  Topic 2: longer awesome set thing course absolutely doubt better potter harry\n",
      "  Topic 3: end cried movie excellent potter harry point\n",
      "  Topic 4: enjoyed didnt came invokes shouldve gose reading magic harry potter\n",
      "  Topic 5: movie reread potter far loved favourite harry\n",
      "\n",
      "\n",
      "Book Title: The Hitchhiker's Guide to the Galaxy (Hitchhiker's Guide to the Galaxy, #1) (Book ID: 11)\n",
      "  Topic 1: intelligent finally sure classic far hilarious lot fish funny 45\n",
      "  Topic 2: eternal insightful favourite im existentialism know race funny life human\n",
      "  Topic 3: humorous dont hitchhiker loved adam galaxy novel humor classic\n",
      "  Topic 4: listening ago funny enjoyable lost liked written audio fun\n",
      "  Topic 5: rereading felt ive im easy laugh currently club enjoyable loved\n",
      "\n",
      "\n",
      "Book Title: The Ultimate Hitchhiker's Guide to the Galaxy (Book ID: 13)\n",
      "  Topic 1: cosmic mangled mutated covered messy explosion leaf charm second funny\n",
      "  Topic 2: coming thing instance big mind quote delightful make laugh hard\n",
      "  Topic 3: odd greatest loud joke comedy science stop fiction probably humour\n",
      "  Topic 4: fantastical quirky entertaining fun bring wish encore trilogy adam author\n",
      "  Topic 5: thats hilarious humor describing ground style awesome year way adam\n",
      "\n",
      "\n",
      "Book Title: The Lord of the Rings (The Lord of the Rings, #1-3) (Book ID: 33)\n",
      "  Topic 1: said writing tolkien ive trilogy movie reading epic lotr say\n",
      "  Topic 2: movie landscape epic earth middle hobbit world reading im mind\n",
      "  Topic 3: omnibus extra ed 3in1 took half nearly loved kid use\n",
      "  Topic 4: movie ring going better ive film hobbit adventure way trilogy\n",
      "  Topic 5: hard follow multitude far plot iti piece work greatest awesome\n",
      "\n",
      "\n",
      "Book Title: The Fellowship of the Ring (The Lord of the Rings, #1) (Book ID: 34)\n",
      "  Topic 1: felt recommend quite novel character ring fully fantasy reading\n",
      "  Topic 2: bored world star tolkien reading way includes year liked movie\n",
      "  Topic 3: favoriteand world ive tolkien complicated lord understand fivestar long know\n",
      "  Topic 4: life fantastic incredibly world writing language audiobook doubt listened year\n",
      "  Topic 5: hobbit based teaching fair think style finish stand listened started\n",
      "\n",
      "\n",
      "Book Title: The Phantom Tollbooth (Book ID: 378)\n",
      "  Topic 1: tollbooth older enjoy younger 36 100 nea educator im childrens\n",
      "  Topic 2: boy reading enjoyed delightful age amazing wish wordplay little\n",
      "  Topic 3: setting 5th educational imagery kidsschool loved think grade image yeah\n",
      "  Topic 4: purge lucky mama survived mother hilarious recently childrens clever alltime\n",
      "  Topic 5: written killer cool day people child funny imaginative classic reread\n",
      "\n",
      "\n",
      "Book Title: A Midsummer Night's Dream (Book ID: 1622)\n",
      "  Topic 1: word grade clearly 7th magical fairy mystical favourite work shakespeare\n",
      "  Topic 2: bye dream enjoyed impression hardback switching person confused thoroughly shakespeare\n",
      "  Topic 3: worth popular consider known lesser bitmore expecting vaguely hype shakespeare\n",
      "  Topic 4: enjoyed different nearly think delight gold flurry fun reading play\n",
      "  Topic 5: compared isnt dont young forest course havent day sooooooooooooo finished\n",
      "\n",
      "\n",
      "Book Title: Anansi Boys (Book ID: 2744)\n",
      "  Topic 1: lenny unique narrator think listened short car enjoyed gaiman way\n",
      "  Topic 2: modern gaiman interesting prize look african gaimans version anansi god\n",
      "  Topic 3: told holt tom style brother neil gaiman american spider fat\n",
      "  Topic 4: inside hilarious little storytelling loved fucken actually american better intriguing\n",
      "  Topic 5: theme dont neil didnt character quite american think im gaiman\n",
      "\n",
      "\n",
      "Book Title: A Great and Terrible Beauty (Gemma Doyle, #1) (Book ID: 3682)\n",
      "  Topic 1: school girl mother dont victorian think fantasy gemma\n",
      "  Topic 2: think enjoy quite doyle wait tried store star teen hmm\n",
      "  Topic 3: took reason toe bit character continue dont dragged liked historical\n",
      "  Topic 4: liked say world teenage commend coming hard dimensional girl soon\n",
      "  Topic 5: plot thing period quite page liked school girl confusing character\n",
      "\n",
      "\n",
      "Book Title: Life of Pi (Book ID: 4214)\n",
      "  Topic 1: reading say beautifully amazing chapter life written adventure pi\n",
      "  Topic 2: getting fantastic enjoyed thought interesting finish stand sea little amazing\n",
      "  Topic 3: quite better concept bit remember character movie different pi want\n",
      "  Topic 4: loved fantastic film felt enjoyed worth start life im novel\n",
      "  Topic 5: worth dont sure wonderful end ending lifeboat tiger pi\n",
      "\n",
      "\n",
      "Book Title: American Gods (American Gods, #1) (Book ID: 4407)\n",
      "  Topic 1: halfway neil httpmizparkerwordpresscom20110 character new american gaiman reading god\n",
      "  Topic 2: enjoyable fucking gaiman phew hard felt 35 long im god\n",
      "  Topic 3: character shadow thought im little concept interesting gaiman definitely god\n",
      "  Topic 4: coming liked novel think gaiman enjoyed character god\n",
      "  Topic 5: character ending sure interesting weird neil american god thought finish\n",
      "\n",
      "\n",
      "Book Title: The Dark Tower (Book ID: 5091)\n",
      "  Topic 1: ending im wonderful happens pretty writing king end dark tower\n",
      "  Topic 2: loved dark people hated know previous piss way reader okay\n",
      "  Topic 3: idea want actually amazing king thing forever hate saga sense\n",
      "  Topic 4: actually ended rest hope opus digest ending magnus excellent bit\n",
      "  Topic 5: wont absolutely id enthusiastically promulgate unfulfilling asked dark tower ending\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Set minimum number of reviews per book\n",
    "min_reviews = 10  \n",
    "\n",
    "# Process only the first 500 books for proof of concept\n",
    "subset_books = df_sampled['book_id'].value_counts().index[:1000]  \n",
    "df_filtered = df_sampled[df_sampled['book_id'].isin(subset_books)]\n",
    "\n",
    "# Define a custom list of generic words to remove\n",
    "custom_stopwords = set([\n",
    "    \"book\", \"read\", \"series\", \"time\", \"love\", \"story\", \"like\", \n",
    "    \"really\", \"good\", \"great\", \"best\", \"favorite\", \"review\"\n",
    "])\n",
    "\n",
    "# Function to process a single book\n",
    "def process_book(book_id, group):\n",
    "    if len(group) < min_reviews:\n",
    "        return book_id, None  # Skip books with too few reviews\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=3000)\n",
    "    X = vectorizer.fit_transform(group['cleaned_review_text'])\n",
    "\n",
    "    lda_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "    lda_model.fit(X)\n",
    "\n",
    "    # Extract topics\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        top_words = [terms[i] for i in topic.argsort()[-15:]]  # Get more words per topic\n",
    "        filtered_words = [word for word in top_words if word not in custom_stopwords]\n",
    "        topics.append(\" \".join(filtered_words[:10]))  # Keep only top 10 after filtering\n",
    "\n",
    "    # Get book title\n",
    "    title = group['title'].iloc[0]  # Assuming all reviews for a book have the same title\n",
    "\n",
    "    return book_id, title, topics\n",
    "\n",
    "# Run topic modeling in parallel\n",
    "book_groups = df_filtered.groupby('book_id')\n",
    "results = Parallel(n_jobs=-1)(delayed(process_book)(book_id, group) for book_id, group in book_groups)\n",
    "\n",
    "# Convert results to dictionary\n",
    "book_topics = {book_id: (title, topics) for book_id, title, topics in results if topics}\n",
    "\n",
    "# Print results for the first 5 books\n",
    "for book_id, (title, topics) in list(book_topics.items())[:15]:\n",
    "    print(f\"Book Title: {title} (Book ID: {book_id})\")\n",
    "    for i, topic in enumerate(topics):\n",
    "        print(f\"  Topic {i+1}: {topic}\")\n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
